input {

	tcp {
		port => 5000
		codec => json
    type => "applicationLogs"
	}

## use jdbc input plugin to get the data from postgres
#	 jdbc {
#        jdbc_driver_class => "org.postgresql.Driver"
#        jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.2.22.jar"
#        jdbc_connection_string => "jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT:5432}/${POSTGRES_DB}"
#        jdbc_validate_connection => true
#        jdbc_user => "${POSTGRES_USER}"
#        jdbc_password => "${POSTGRES_PASSWORD}"
#        schedule => "* * * * *"
#        statement => "SELECT * from perfumes"
#		     jdbc_paging_enabled => "true"
#        jdbc_page_size => "300"
#  }

}

## Add your filters / logstash plugins configuration here
output {
    ## used to output the values in the terminal (DEBUGGING)
    ## once everything is working, comment out this line
    # stdout { codec => "json" }

## use the tcp plugin  for writing logs to an elasticsearch index
## the commented configuration is for conditionally getting the postgres data using the jdbc plugin into an elasticsearch index
#   if [type] == "applicationLogs" {
      elasticsearch {
        hosts => "elasticsearch:${ELASTICSEARCH_PORT}"
        user => " ${ELASTIC_USERNAME}"
        password => "${ELASTIC_PASSWORD}"
        }
#   } else {
#       elasticsearch {
#        hosts => "elasticsearch:${ELASTICSEARCH_PORT}"
#        user => " ${ELASTIC_USERNAME}"
#        password => "${ELASTIC_PASSWORD}"
#        index => "scentdbindex"
#        document_id => "document_%{pg_table_column_id}"
#        doc_as_upsert => true # upserts documents (e.g. if the document does not exist, creates a new record)
#      }
#    }

}
